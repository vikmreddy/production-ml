{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_input_vector(img_name, numcontext):\n",
    "    '''\n",
    "    Turn an audio file into feature representation.\n",
    "\n",
    "    This function has been modified from Mozilla DeepSpeech:\n",
    "    https://github.com/mozilla/DeepSpeech/blob/master/util/audio.py\n",
    "\n",
    "    # This Source Code Form is subject to the terms of the Mozilla Public\n",
    "    # License, v. 2.0. If a copy of the MPL was not distributed with this\n",
    "    # file, You can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "    '''\n",
    "\n",
    "    # Load image file\n",
    "    # DONT FORGET THE CONVERT L!!!! OTHERWISE there are only 3 channels gr...\n",
    "    im = np.array(Image.open(img_name).convert('L')\n",
    "                  , dtype=np.uint32)\n",
    "    #print(im.shape)\n",
    "    orig_inputs = im.transpose()\n",
    "                  \n",
    "    # Get mfcc coefficients\n",
    "    #orig_inputs = mfcc(audio, samplerate=fs, numcep=numcep)\n",
    "    #print(orig_inputs.shape)\n",
    "    # We only keep every second feature (BiRNN stride = 2)\n",
    "    orig_inputs = orig_inputs[::2]\n",
    "    \n",
    "    # numcep is my # of rows of pixels in the line\n",
    "    numcep = orig_inputs.shape[1]\n",
    "    \n",
    "    # For each time slice of the training set, we need to copy the context this makes\n",
    "    # the numcep dimensions vector into a numcep + 2*numcep*numcontext dimensions\n",
    "    # because of:\n",
    "    #  - numcep dimensions for the current mfcc feature set\n",
    "    #  - numcontext*numcep dimensions for each of the past and future (x2) mfcc feature set\n",
    "    # => so numcep + 2*numcontext*numcep\n",
    "    train_inputs = np.array([], np.float32)\n",
    "    train_inputs.resize((orig_inputs.shape[0], numcep + 2 * numcep * numcontext))\n",
    "\n",
    "    # Prepare pre-fix post fix context\n",
    "    empty_column = np.array([])\n",
    "    empty_column.resize((numcep))\n",
    "\n",
    "    # Prepare train_inputs with past and future contexts\n",
    "    time_slices = range(train_inputs.shape[0])\n",
    "    context_past_min = time_slices[0] + numcontext\n",
    "    context_future_max = time_slices[-1] - numcontext\n",
    "    for time_slice in time_slices:\n",
    "        # Reminder: array[start:stop:step]\n",
    "        # slices from indice |start| up to |stop| (not included), every |step|\n",
    "\n",
    "        # Add empty context data of the correct size to the start and end\n",
    "        # of the MFCC feature matrix\n",
    "\n",
    "        # Pick up to numcontext time slices in the past, and complete with empty\n",
    "        # mfcc features\n",
    "        need_empty_past = max(0, (context_past_min - time_slice))\n",
    "        empty_source_past = list(empty_column for empty_slots in range(need_empty_past))\n",
    "        #print(len(empty_source_past))\n",
    "        data_source_past = orig_inputs[max(0, time_slice - numcontext):time_slice]\n",
    "        #print(orig_inputs.shape)\n",
    "        #print(data_source_past.shape)\n",
    "        assert(len(empty_source_past) + len(data_source_past) == numcontext)\n",
    "\n",
    "        # Pick up to numcontext time slices in the future, and complete with empty\n",
    "        # mfcc features\n",
    "        need_empty_future = max(0, (time_slice - context_future_max))\n",
    "        empty_source_future = list(empty_column for empty_slots in range(need_empty_future))\n",
    "        data_source_future = orig_inputs[time_slice + 1:time_slice + numcontext + 1]\n",
    "        assert(len(empty_source_future) + len(data_source_future) == numcontext)\n",
    "\n",
    "        if need_empty_past:\n",
    "            past = np.concatenate((empty_source_past, data_source_past))\n",
    "        else:\n",
    "            past = data_source_past\n",
    "\n",
    "        if need_empty_future:\n",
    "            future = np.concatenate((data_source_future, empty_source_future))\n",
    "        else:\n",
    "            future = data_source_future\n",
    "\n",
    "        past = np.reshape(past, numcontext * numcep)\n",
    "        now = orig_inputs[time_slice]\n",
    "        future = np.reshape(future, numcontext * numcep)\n",
    "\n",
    "        train_inputs[time_slice] = np.concatenate((past, now, future))\n",
    "        assert(len(train_inputs[time_slice]) == numcep + 2 * numcep * numcontext)\n",
    "\n",
    "    # Scale/standardize the inputs\n",
    "    # This can be done more efficiently in the TensorFlow graph\n",
    "    train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n",
    "    return train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_inputs = image_to_input_vector('hindi_data/hindi_lines/line_0.jpg', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_and_transcript(txt_files, img_files, n_context):\n",
    "    '''\n",
    "    Loads image files and text transcriptions from ordered lists of filenames.\n",
    "    Converts to images to df arrays and text to numerical arrays.\n",
    "    Returns list of arrays. Returned image array list can be padded with\n",
    "    pad_sequences function in this same module.\n",
    "    '''\n",
    "    image = []\n",
    "    image_len = []\n",
    "    transcript = []\n",
    "    transcript_len = []\n",
    "\n",
    "    for txt_file, img_file in zip(txt_files, img_files):\n",
    "        # load audio and convert to features\n",
    "        image_data = image_to_input_vector(img_file, n_context)\n",
    "        image_data = image_data.astype('float32')\n",
    "\n",
    "        image.append(image_data)\n",
    "        image_len.append(np.int32(len(image_data)))\n",
    "\n",
    "        # load text transcription and convert to numerical array\n",
    "        #target = normalize_txt_file(txt_file)\n",
    "        #target = text_to_char_array(target)\n",
    "        #transcript.append(target)\n",
    "        #transcript_len.append(len(target))\n",
    "    print('i',image)\n",
    "    image = np.asarray(image)\n",
    "    print('i2',image)\n",
    "    image_len = np.asarray(image_len)\n",
    "    transcript = np.asarray(transcript)\n",
    "    transcript_len = np.asarray(transcript_len)\n",
    "    return image, image_len, transcript, transcript_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i [array([[-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
      "         0.19210966,  0.19210966],\n",
      "       [-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
      "         0.19210966,  0.19210966],\n",
      "       [-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
      "         0.19210966,  0.19210966],\n",
      "       ..., \n",
      "       [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
      "        -5.29195452, -5.29195452],\n",
      "       [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
      "        -5.29195452, -5.29195452],\n",
      "       [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
      "        -5.29195452, -5.29195452]], dtype=float32), array([[-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
      "         0.19210966,  0.19210966],\n",
      "       [-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
      "         0.19210966,  0.19210966],\n",
      "       [-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
      "         0.19210966,  0.19210966],\n",
      "       ..., \n",
      "       [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
      "        -5.29195452, -5.29195452],\n",
      "       [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
      "        -5.29195452, -5.29195452],\n",
      "       [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
      "        -5.29195452, -5.29195452]], dtype=float32)]\n",
      "i2 [[[-5.29195452 -5.29195452 -5.29195452 ...,  0.19210966  0.19210966\n",
      "    0.19210966]\n",
      "  [-5.29195452 -5.29195452 -5.29195452 ...,  0.19210966  0.19210966\n",
      "    0.19210966]\n",
      "  [-5.29195452 -5.29195452 -5.29195452 ...,  0.19210966  0.19210966\n",
      "    0.19210966]\n",
      "  ..., \n",
      "  [ 0.19210966  0.19210966  0.19210966 ..., -5.29195452 -5.29195452\n",
      "   -5.29195452]\n",
      "  [ 0.19210966  0.19210966  0.19210966 ..., -5.29195452 -5.29195452\n",
      "   -5.29195452]\n",
      "  [ 0.19210966  0.19210966  0.19210966 ..., -5.29195452 -5.29195452\n",
      "   -5.29195452]]\n",
      "\n",
      " [[-5.29195452 -5.29195452 -5.29195452 ...,  0.19210966  0.19210966\n",
      "    0.19210966]\n",
      "  [-5.29195452 -5.29195452 -5.29195452 ...,  0.19210966  0.19210966\n",
      "    0.19210966]\n",
      "  [-5.29195452 -5.29195452 -5.29195452 ...,  0.19210966  0.19210966\n",
      "    0.19210966]\n",
      "  ..., \n",
      "  [ 0.19210966  0.19210966  0.19210966 ..., -5.29195452 -5.29195452\n",
      "   -5.29195452]\n",
      "  [ 0.19210966  0.19210966  0.19210966 ..., -5.29195452 -5.29195452\n",
      "   -5.29195452]\n",
      "  [ 0.19210966  0.19210966  0.19210966 ..., -5.29195452 -5.29195452\n",
      "   -5.29195452]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
       "           0.19210966,  0.19210966],\n",
       "         [-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
       "           0.19210966,  0.19210966],\n",
       "         [-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
       "           0.19210966,  0.19210966],\n",
       "         ..., \n",
       "         [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
       "          -5.29195452, -5.29195452],\n",
       "         [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
       "          -5.29195452, -5.29195452],\n",
       "         [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
       "          -5.29195452, -5.29195452]],\n",
       " \n",
       "        [[-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
       "           0.19210966,  0.19210966],\n",
       "         [-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
       "           0.19210966,  0.19210966],\n",
       "         [-5.29195452, -5.29195452, -5.29195452, ...,  0.19210966,\n",
       "           0.19210966,  0.19210966],\n",
       "         ..., \n",
       "         [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
       "          -5.29195452, -5.29195452],\n",
       "         [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
       "          -5.29195452, -5.29195452],\n",
       "         [ 0.19210966,  0.19210966,  0.19210966, ..., -5.29195452,\n",
       "          -5.29195452, -5.29195452]]], dtype=float32),\n",
       " array([1301, 1301], dtype=int32),\n",
       " array([], dtype=float64),\n",
       " array([], dtype=float64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image_and_transcript(['hindi_data/hindi_lines/line_0',\n",
    "                         'hindi_data/hindi_lines/line_1'],\n",
    "                         ['hindi_data/hindi_lines/line_0.jpg',\n",
    "                         'hindi_data/hindi_lines/line_1.jpg'],\n",
    "                        4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_image_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_to_input_vector(audio_filename, numcontext):\n",
    "    '''\n",
    "    Turn an audio file into feature representation.\n",
    "\n",
    "    This function has been modified from Mozilla DeepSpeech:\n",
    "    https://github.com/mozilla/DeepSpeech/blob/master/util/audio.py\n",
    "\n",
    "    # This Source Code Form is subject to the terms of the Mozilla Public\n",
    "    # License, v. 2.0. If a copy of the MPL was not distributed with this\n",
    "    # file, You can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "    '''\n",
    "\n",
    "    # Load image file\n",
    "    # DONT FORGET THE CONVERT L!!!! OTHERWISE there are only 3 channels gr...\n",
    "    im = np.array(Image.open('hindi_data/hindi_lines/line_0.jpg').convert('L')\n",
    "                  , dtype=np.uint32)\n",
    "    #print(im.shape)\n",
    "    orig_inputs = im.transpose()\n",
    "                  \n",
    "    # Get mfcc coefficients\n",
    "    #orig_inputs = mfcc(audio, samplerate=fs, numcep=numcep)\n",
    "    #print(orig_inputs.shape)\n",
    "    # We only keep every second feature (BiRNN stride = 2)\n",
    "    orig_inputs = orig_inputs[::2]\n",
    "    \n",
    "    # numcep is my # of rows of pixels in the line\n",
    "    numcep = orig_inputs.shape[1]\n",
    "    \n",
    "    # For each time slice of the training set, we need to copy the context this makes\n",
    "    # the numcep dimensions vector into a numcep + 2*numcep*numcontext dimensions\n",
    "    # because of:\n",
    "    #  - numcep dimensions for the current mfcc feature set\n",
    "    #  - numcontext*numcep dimensions for each of the past and future (x2) mfcc feature set\n",
    "    # => so numcep + 2*numcontext*numcep\n",
    "    train_inputs = np.array([], np.float32)\n",
    "    train_inputs.resize((orig_inputs.shape[0], numcep + 2 * numcep * numcontext))\n",
    "\n",
    "    # Prepare pre-fix post fix context\n",
    "    empty_column = np.array([])\n",
    "    empty_column.resize((numcep))\n",
    "\n",
    "    # Prepare train_inputs with past and future contexts\n",
    "    time_slices = range(train_inputs.shape[0])\n",
    "    context_past_min = time_slices[0] + numcontext\n",
    "    context_future_max = time_slices[-1] - numcontext\n",
    "    for time_slice in time_slices:\n",
    "        # Reminder: array[start:stop:step]\n",
    "        # slices from indice |start| up to |stop| (not included), every |step|\n",
    "\n",
    "        # Add empty context data of the correct size to the start and end\n",
    "        # of the MFCC feature matrix\n",
    "\n",
    "        # Pick up to numcontext time slices in the past, and complete with empty\n",
    "        # mfcc features\n",
    "        need_empty_past = max(0, (context_past_min - time_slice))\n",
    "        empty_source_past = list(empty_column for empty_slots in range(need_empty_past))\n",
    "        #print(len(empty_source_past))\n",
    "        data_source_past = orig_inputs[max(0, time_slice - numcontext):time_slice]\n",
    "        #print(orig_inputs.shape)\n",
    "        #print(data_source_past.shape)\n",
    "        assert(len(empty_source_past) + len(data_source_past) == numcontext)\n",
    "\n",
    "        # Pick up to numcontext time slices in the future, and complete with empty\n",
    "        # mfcc features\n",
    "        need_empty_future = max(0, (time_slice - context_future_max))\n",
    "        empty_source_future = list(empty_column for empty_slots in range(need_empty_future))\n",
    "        data_source_future = orig_inputs[time_slice + 1:time_slice + numcontext + 1]\n",
    "        assert(len(empty_source_future) + len(data_source_future) == numcontext)\n",
    "\n",
    "        if need_empty_past:\n",
    "            past = np.concatenate((empty_source_past, data_source_past))\n",
    "        else:\n",
    "            past = data_source_past\n",
    "\n",
    "        if need_empty_future:\n",
    "            future = np.concatenate((data_source_future, empty_source_future))\n",
    "        else:\n",
    "            future = data_source_future\n",
    "\n",
    "        past = np.reshape(past, numcontext * numcep)\n",
    "        now = orig_inputs[time_slice]\n",
    "        future = np.reshape(future, numcontext * numcep)\n",
    "\n",
    "        train_inputs[time_slice] = np.concatenate((past, now, future))\n",
    "        assert(len(train_inputs[time_slice]) == numcep + 2 * numcep * numcontext)\n",
    "\n",
    "    # Scale/standardize the inputs\n",
    "    # This can be done more efficiently in the TensorFlow graph\n",
    "    train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n",
    "    return train_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
